\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\theoremstyle{definition}
\newtheorem{remark}{Remark}[section]
%-------------------------------------- 
\begin{document}

\title{Math Problem Set 2}
\author{Matthew Brown\\ 
OSM Boot Camp 2018} %if necessary, replace with your course title
 
\maketitle
 
\begin{problem}{3.1} There are two parts: 
\begin{itemize}
\item (i)
\begin{align*}
||x+y||^2 -||x-y||^2 &= \langle x+y,x+y \rangle - \langle x-y,x-y \rangle \\
&= \langle x+y,x \rangle + \langle x+y,y \rangle - (\langle x-y, x \rangle + \langle x-y, -y\rangle) \\
&= \langle x,x+y \rangle + \langle y,x+y \rangle - (\langle x, x-y \rangle + \langle -y,x-y\rangle \\
&= \langle x, x \rangle + \langle x, y\rangle + \langle y,x \rangle + \langle y,y \rangle - \langle x, x \rangle - \langle x, -y \rangle - \langle -y,x \rangle - \langle -y,-y \rangle \\
&= 4\langle x,y \rangle \text{ after some easy manipulations} 
\end{align*}
So it is clear that $\frac{1}{4}(||x+y||^2 -||x-y||^2) = \langle x, y \rangle $
\item (ii)
As above, 
\begin{align*}
||x+y||^2 + ||x-y||^2 &= \langle x+y,x+y \rangle + \langle x-y,x-y \rangle \\
&= \langle x+y,x \rangle + \langle x+y,y \rangle + (\langle x-y, x \rangle + \langle x-y, -y\rangle) \\
&= \langle x,x+y \rangle + \langle y,x+y \rangle + (\langle x, x-y \rangle + \langle -y,x-y\rangle \\
&= \langle x, x \rangle + \langle x, y\rangle + \langle y,x \rangle + \langle y,y \rangle + \langle x, x \rangle + \langle x, -y \rangle + \langle -y,x \rangle + \langle -y,-y \rangle \\
&= 2 \langle x, x \rangle + 2 \langle y, y, \rangle \text{ after some easy manipulations} \\
&= 2(||x||^2 + ||y||^2) 
\end{align*}
So it is clear that $\frac{1}{2}(||x+y||^2 + ||x-y||^2) = ||x||^2 + ||y||^2$
\end{itemize}
\end{problem}

\begin{problem}{3.2}
I proceed similarly: just start from the left side, expand, and simplify.
\end{problem}

\begin{problem}{3.3}
Let $\theta$ be the angle in question. Recall that $$ cos(\theta) - \frac{\langle f, g \rangle}{||f|| \cdot ||g||} $$
\begin{itemize}
\item (i)
\begin{align*}
&\langle f,g \rangle = \int_0^1 fg dx = \int_0^1 x^6 dx = \bigg(\frac{x^7}{7} \bigg) \bigg\vert_0^1 = \frac{1}{7} \\
&||f||^2 = \int_0^1 f^2 dx = \int_0^1 x^2 dx = \bigg(\frac{x^3}{3} \bigg) \bigg\vert_0^1 = \frac{1}{3} \\
&||g||^2 = \int_0^1 g^2 dx = \int_0^1 x^10 dx = \bigg(\frac{x^{11}}{11}\bigg) \bigg\vert_0^1 = \frac{1}{11}
\end{align*}
Thus, we see that 
\begin{equation}
cos(\theta) = \frac{\frac{1}{7}}{(\frac{1}{3} \cdot \frac{1}{11}) ^\frac{1}{2}} = \frac{33^{\frac{1}{2}}}{7}
\end{equation}
And (1) implies that $\theta \approx 35^{\circ}$. 
\item (ii)
\begin{align*}
&\langle f,g \rangle = \int_0^1 fg dx = \int_0^1 x^6 dx = \bigg(\frac{x^7}{7} \bigg) \bigg\vert_0^1 = \frac{1}{7} \\
&||f||^2 = \int_0^1 f^2 dx = \int_0^1 x^4 dx = \bigg(\frac{x^5}{8} \bigg) \bigg\vert_0^1 = \frac{1}{5} \\
&||g||^2 = \int_0^1 g^2 dx = \int_0^1 x^8 dx = \bigg(\frac{x^{9}}{9}\bigg) \bigg\vert_0^1 = \frac{1}{9}
\end{align*}
Thus, we see that 
\begin{equation}
cos(\theta) = \frac{\frac{1}{7}}{(\frac{1}{5} \cdot \frac{1}{9}) ^\frac{1}{2}} = \frac{45^{\frac{1}{2}}}{7}
\end{equation}
And (1) implies that $\theta \approx 17^{\circ}$
\end{itemize}
\end{problem}

\begin{problem}{3.8} There are four parts
\begin{itemize}
\item (i)
\begin{proof}
This is just a matter of checking all the relevant details. \\
Norms = 1:
\begin{align*}
&||cos(t)||^2 = \frac{1}{\pi} \int_{-\pi}^{\pi} cos^2(t) dt = \frac{1}{\pi}\frac{cos(x)sin(x) + x}{2} \big\vert^{\pi}_{-\pi} = 1 \\
&||sin(t)||^2 = \frac{1}{\pi} \int_{-\pi}^{\pi} sin^2(t) dt = \frac{1}{\pi}\frac{-sin(2x)+2x}{4} \big\vert^{\pi}_{-\pi} = 1 \\
&||cos(2t)||^2 = \frac{1}{\pi} \int_{-\pi}^{\pi} cos^2(2t) dt = \frac{1}{\pi}\frac{sin(4x)+4x}{8} \big\vert^{\pi}_{-\pi} = 1 \\
&||sin(2t)||^2 = \frac{1}{\pi} \int_{-\pi}^{\pi} sin^2(2t) dt = \frac{1}{\pi}\frac{-sin(4x)+4x}{8} \big\vert^{\pi}_{-\pi} = 1
\end{align*}
Inner Products = 1:
\begin{align*}
&\langle cos(t), sin(t) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} cos(t)sin(t) dt = \frac{sin^2(x)}{2} \big\vert^{\pi}_{-\pi} = sin^2(\pi) - sin^2(-\pi) = 0 
\end{align*}
The proof is completed by checking all the other inner products similarly.
\end{proof}
\item (ii) 
\begin{align*}
&||t||^2 = \int_{-\pi}^{\pi} t^2 dt = \frac{t^3}{3} \big\vert^{\pi}_{-\pi} = \frac{\pi^3}{3} - \frac{(-\pi)^3}{3} = \frac{2\pi^3}{3} \\
&||t|| = \bigg( \frac{2\pi^3}{3} \bigg)^{\frac{1}{2}}
\end{align*}
\item(iii) 
\begin{align*}
proj_X(cos(3t)) &= \langle sin(t), cos(3t)\rangle sin(t) + \langle cos(t), cos(3t) \rangle cos(t) \\
&+ \langle sin(2t), cos(3t) \rangle sin(2t) + \langle cos(2t), cos(3t) \rangle cos(2t) \\
&= 0 + 0 + 0 + 0 = 0
\end{align*}
(We see that $cos(3t)$ is orthogonal to $X$)
\item (iv) 
\begin{align*}
proj_X(t) = 0 + 2sin(t) + 0 - sin(2t)
\end{align*}
\end{itemize}
\end{problem}

\begin{problem}{3.9} 
\begin{proof}
In $\mathbb{R}^2$, a rotation about the origin by arbitrary angle $\theta$ can be described by the matrix 
$$
M = 
\begin{bmatrix}
cos(\theta) & - sin(\theta) \\
sin(\theta) & cos(\theta)
\end{bmatrix}
$$
So that $M\bigg(\begin{pmatrix} x \\ y \end{pmatrix}\bigg) = (\begin{smallmatrix} xcos(\theta) - ysin(\theta) \\ xsin(\theta) + ycos(\theta) \end{smallmatrix}) $.
Let $a = (\begin{smallmatrix} a_1 \\ a_2 \end{smallmatrix}), b= (\begin{smallmatrix} b_1 \\ b_2 \end{smallmatrix}) \in \mathbb{R}^2$. Then 
\begin{align*}
\langle M(a), M(b) \rangle &= \bigg\langle \begin{pmatrix} a_1cos(\theta) - a_2sin(\theta) \\ a_1sin(\theta) + a_2cos(\theta) \end{pmatrix}, \begin{pmatrix} b_1cos(\theta) - b_2sin(\theta) \\ b_1sin(\theta) + b_2cos(\theta) \end{pmatrix} \bigg\rangle \\
&= a_1b_1cos^2(\theta)+a_2b_2sin^2(\theta) + a_1b_1sin^2(\theta) + a_2b_2cos^2(\theta \\
&= a_1b_1+a_2b_2 = \langle a, b \rangle
\end{align*}
So we see that $M$ is an orthonormal operator. (N.b that this is a terribly inefficient way to prove this - I should have just shown that the columns of $M$ were orthonormal!
\end{proof}
\end{problem}
\begin{problem}{3.10} Recall that taking the Hermitian is flipping rows and columns and taking the conjugate.
\begin{itemize}
\item (i) \begin{proof}
We need to show both directions. \\
$\Rightarrow$: Let $Q$ be an orthonormal matrix. Then $\langle m, n \rangle = \langle Qm, Qn \rangle \implies m^H n = (Qm)^H Qn = m^H Q^H Qn.$ And because $m$ and $n$ were arbitrarily chosen, the only way that this equality holds is if $Q^HQ = I$, and this gives us that $QQ^H = Q$ since left inverse $\implies$ right inverse.  \\
$\Leftarrow$: Let $Q$ be a matrix so that $Q^HQ = I$. Then consider $\langle Qm, Qn \rangle = (Qm)^H Qn = m^HQ^HQn = m^Hn = \langle m, n \rangle$.
\end{proof}
\item (ii)
\begin{proof} This is pretty easy:
$$
||x|| = \sqrt[2]{\langle x, x \rangle} = \sqrt[2]{\langle x, x \rangle} = ||Qx||
$$
\end{proof}
\item (iii) \begin{proof}
Assume Q is orthonormal. Then $QQ^H=Q^HQ = I \implies Q^H = Q^{-1}$. I'll prove the following short lemma.
\begin{lemma} For $Q$ orthonormal, $Q^H$ is orthonormal.
\end{lemma}
Recall that $(Q^H)^H = Q$, and see that 
$$
(Q^H)^HQ^H = Q^H(Q^H)^H = I 
$$ 
which proves the lemma. \\
And since $Q^{-1} = Q^H$, $Q^{-1}$ is orthonormal. 
\end{proof}
\item (iv) 
\begin{proof}
We'll examine the elements of the identity matrix element by element. First note that: 
$$
I_{ij} = \delta_{ij} = \begin{cases} 1 \text{ if } i = j \\ 
0 \text{ if } i \neq j 
\end{cases}
$$ Then we'll compare this to what we get when we multiply $QQ^H$, which we know is equal to $I$ in all its coordinates. First, though, for any matrix $A$, define $A^i$ to be the "ith row" of $A$ and $A_j$ to be the jth column. Then
$$ \delta_{ij} = (Q^HQ)_{ij} = (Q^H)^i Q_j = $$
Recall now that $(Q^H)^i = \bar{Q_i}$, by definition of the Hermitian. But now we see that 
$$\langle \bar{Q_i}, q_j \rangle = \delta_{ij} $$
and the columns of $Q$ are orthonormal.
\end{proof}
\item (v) 
A counterexample would be the matrix $M = \begin{smallmatrix} 2 & 0 \\ 0 & \frac{1}{2} \end{smallmatrix} $. det $B = 1$, but $Be_1 = 2e_1$ and $||e_1|| \neq ||2e_1|| = ||Be_1||$ which violates what we proved in (ii).
\item(vi) \begin{proof}
This is also quite short:

$$
(Q_1Q_2)(Q_1Q_2)^H = Q_1Q_2Q_2^HQ_1^H = Q_1Q_1^H = I
$$
And we also get the left inverse by properties of inverses.
\end{proof}
\end{itemize}
\end{problem}
\begin{problem}{3.11}
Suppose that $x_1, ... , x_n$ is as et of linearly $\textit{dependent}$ vectors. Let's apply Gram-Schmidt. Eventually, we will arrive at a vector $x_k$ which is linearly dependent upon $x_1, ..., x_{k-1}$. But then, if $X = span(x_1, ..., x_{k-1}) $, then $x_k \in X$ and $p_{k-1} = proj_X(x_k) = x_k$, which forces $q_k = 0$. In the end, if we throw out all these zeroes, we still get an orthonormal basis $q_1, ... q_m$ of $X$ where $m = \text{dim} X$. 
\end{problem}


\begin{problem}{3.16} Done with Albi
\end{problem}

\begin{problem}{3.17} \begin{proof}
\begin{align*}
\hat{R}x = \hat{Q}^Hb \\
\iff \hat{Q} \hat{R} x = b \\
\iff Ax = b \\
\iff A^H Ax = A^H b
\end{align*}
Note that THIS IS NOT THE CORRECT PROOF AND I NEED TO FIX THIS
\end{proof}
\end{problem}

\begin{problem}{3.23}doesn't look bad
\end{problem}

\begin{problem}{3.24}Procedural (show things are norms)
\end{problem}

\begin{problem}{3.26} 
First I must show that topological equivalence is an equivalence relation.
\begin{proof}
REST OF THIS DONE WITH ALBI SEE GIT
I must show three things: (i) $x \sim x$. (ii) $x \sim y  \implies y \sim x$, (iii) $x \sim y$ and $y \sim z \implies x \sim z$. \\
(i) $ || \cdot ||_1 \sim || \cdot ||_1 $ trivially. Let $M \geq m$, then $m||x||_1 \leq || x ||_1 \leq M|| x ||_1$ for all x. \\
(ii) Also trivial: Suppose $|| \cdot ||_1 \sim || \cdot ||_2$. Then $m||x||_1 \leq || x ||_2 \leq M|| x ||_1$ for all x, which implies that $M^{-1}||x||_2 \leq ||x||_1 \leq m^{-1}||x||_2$. \\
(iii) Suppose $|| \cdot ||_1 \sim || \cdot ||_2 $, and $|| \cdot ||_2 \sim || \cdot ||_3$. Then $m||x||_1 \leq || x ||_2 \leq M|| x ||_1$ and $n||x||_2 \leq || x ||_3 \leq N|| x ||_2$. But we get from this that $mn||x||_1 \leq ||x||_3 \leq MN||x||_1$.
\end{proof}
Now I'll show that the $1, 2$, and $ \infty$ norms are topologically equivalent. 
\begin{proof}
(i) $|| \cdot ||_1 \sim || \cdot ||_2$: \\
If we think about the inner product as the standard dot-product, then we have $$(||x||_1)^2 = \sum_{i=1}^n \sum_{j=1}^n |x_i||x_j| \geq \sum_{i=1}^n x_i^2 = \langle x, x, \rangle = (||x||_2)^2$$ 
(the inequality comes because we simply threw out some positive terms on the left side). This implies that $||x||_1 \geq ||x||_2$. Moreover,
$$
(\sqrt[2]{n} ||x||_2)^2 = n \sum_{i=1}^n x_i 
$$

(ii) $|| \cdot ||_\infty \sim || \cdot ||_2$\
$$
||x||_\infty = max_{1 \leq i \leq n}\{ x_i \} = \sqrt[2]{(max_{1 \leq i \leq n} \{ x_i \})^2} \leq \sqrt[2]{\sum_{i=1}^n x_i} = ||x||_2
$$
\end{proof}
\end{problem}

\begin{problem}{3.28}
Related to Previous
\end{problem}

\begin{problem}{3.29}
I will prove two statements. \\

\textbf{The norm of an orthonormal matrix is 1:}
\begin{proof}
Let $Q$ be an orthonormal matrix. Then 
$$ ||Qx|| = ||x|| \implies sup_{x\neq 0} \frac{||Qx||}{||x||} = ||Q|| = 1 $$
\end{proof}

\textbf{If $R_x : M_n(\mathbb{F}) \to  \mathbb{F} , R_x(A) = Ax$, then $||R_x||=||x||$}:
\begin{proof}
The first step is to show $||R_x|| < ||x||$. $$||R_x|| = sup_{A\neq 0} \frac{||R_x(A)||}{||A||} = sup_{A\neq 0} \frac{||Ax||}{||A||} = sup_{A\neq 0} \frac{||Ax||\cdot ||x||}{||A||\cdot ||x||}$$
By Remark 3.5.12, $||Ax|| \leq ||A|| \cdot ||x|| \forall x \in \mathbb{F}^n$, so $$||R_x|| =sup_{A \neq 0} \frac{||Ax||\cdot ||x||}{||A||\cdot ||x||} \leq sup_{A \neq 0}\frac{||Ax|| \cdot ||x||}{||Ax||} = ||x|| $$
Now I'll show equality. For the $\leq$ above to be strict, we must have $ ||Ax|| < ||A||\cdot||A|| $ for all operators $A$ (because we're taking the supremum). $||x|| > 0$, so I can rearrange for the condition: 
$$ \frac{||Ax||}{||x||} < ||A||, \text{ for all operators } A, \text{ vectors } x $$
In other words, no $x$ achieves the supremum which is encoded in the definition of $||A||$. I will use the previous result to show that this will never hold. \\
Let $q_1 = e_1$ (or some other vector with norm 1). I can use the gram-schmidt algorithm to construct an orthonormal basis $q_1, ... q_n$ for $\mathbb{F}^n$. Let $Q$ be the matrix with these basis vectors as its columns. Then $Q$ is an orthonormal matrix. Specifically, $||Q|| = 1$ and it achieves $\frac{||Qx||}{||x||} = ||Q|| = 1$ at all nonzero $x$. \\
This shows that the inequality can never be strict, so we have $||R_x|| = ||x||$
\end{proof} 
\end{problem}

\begin{problem}{3.30}
Show something is a norm (not bad)
\end{problem}

\begin{problem}{3.37}
The first thing is to define the standard basis, which is $\mathcal{B} = \{1, x, x^2\}$ Evaluate $L$ on the basis vectors: 
$$L(1) = 0, L(x) = 1, L(x^2) = 2$$
Now, for $p \in V$, $p$ can be written as a linear combination of these basis vectors. So 
$$ L(p) = L(a_11 + a_2x + a_3x^2) = a_1L(1) +a_2 L(x) + a_3 L(x^2) = \langle (L(1)\cdot 1, L(x), L(x^2)) , (a_1, a_2, a_3) \rangle$$
which is the idea behind the Riesz Representation theorem. So we see that in this case, $q = (0, 1, 2)$... which squares with  what we know about derivatives.
\end{problem}

\begin{problem}{3.38} Let $\mathcal{B}$ as above.
$$
D = \begin{bmatrix}
0 & 1 & 0\\
0 & 0 & 2\\
0 & 0 & 0\\
\end{bmatrix}
$$
Odd - I want to apply the algorithm from 3.7.9 but I can't figure out why the fg term disappears!
\end{problem}

\begin{problem}{3.39}
There are 4 things to show.
\begin{itemize}
\item (i)
\begin{proof}
\begin{align*}
\langle (S+T) v, w \rangle = \langle Sv, w \rangle + \langle Tv, w \rangle = \langle v, S^*w \rangle + \langle v, T^*w \rangle = \langle v, (S^* + T^*)w \rangle \\
\langle \alpha T^*v, w \rangle = \alpha \langle Tv, w \rangle= \alpha \langle v, T^*w \rangle = \langle v, \overline{\alpha}T^*w \rangle
\end{align*}
\end{proof}
\item (ii)
\begin{proof}
$$ \langle S^*v, w \rangle = \overline{\langle w, S^*v \rangle} = \overline{\langle Sw, v \rangle} = \langle v, Sw \rangle $$
\end{proof}
\item (iii)
\begin{proof}
$$\langle STv, w \rangle = \langle Tv, S^*w \rangle = \langle v, T^*S^*w \rangle$$
\end{proof}
\item (iv)
\begin{proof}
Consider the composition $T^*(T^{-1})^*$. 
$$
\langle 
T^*(T^{-1})^*x, y \rangle = \langle (T^{-1})^*x, Ty \rangle= \langle x, (T^{-1})Ty \rangle = \langle x, y \rangle
$$
Since the above is true for all $x, y$, we must have $T^*(T^{-1})^* = I $ 
\end{proof}
\end{itemize}
\end{problem}

\begin{problem}{3.40}
\end{problem}
\begin{itemize}
\item (i) (Considering $A$ as the operator)
$$
\langle AB, C \rangle = \text{ tr }(AB)^HC = \text{ tr } B^HA^HC = \langle B, A^HC \rangle
$$
\item (ii) (seems easy!)
\end{itemize}

\begin{problem}{3.44}
\begin{proof}
 By the fundamental subspaces theorem, Ker$(A^H) =$ Range$(A)$. So we can reformulate the second possibility to: there exists $y \in \text{ Range}(A)^\perp : \langle y, b \rangle \neq 0 $. \\
 Consider now $p = proj_{\text{Range}(A)}b$. If $p = b$, then $b \in$ Range$(A)$ and we have the first case. Otherwise, the procedure creates a residual vector $r$, $r = b - p$. $r \in \text{ Range}(A)^\perp$, and 
$$\langle r, b \rangle = \overline{\langle p + r, r \rangle} = \overline{\langle p, r \rangle} + \overline{\langle r, r \rangle} = \langle p, b \rangle + \langle r, r \rangle = \langle r, r \rangle \neq 0$$
which is the second case. 
\end{proof}
\end{problem}

\begin{problem}{3.45}
\end{problem}

\begin{problem}{3.47}
\end{problem}

\begin{problem}{3.48}
\end{problem}

\begin{problem}{3.50}
\end{problem}

\end{document}
