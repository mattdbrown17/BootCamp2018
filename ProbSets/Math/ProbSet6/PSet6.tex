\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}

\graphicspath{ {C:/Users/mattd/OneDrive/Documents/LatexImages/OSM2018/MathPS5/} }

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}} 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\theoremstyle{definition}
\newtheorem{remark}{Remark}[section]
%-------------------------------------- 
\begin{document}

\title{Math Problem Set 6}
\author{Matthew Brown\\ 
OSM Boot Camp 2018} %if necessary, replace with your course title
 
\maketitle
 
\begin{problem}{9.1}
\begin{proof}
Let $L$ be an unconstrained linear objective function. Suppose that $L$ has a minimizer $x^*$. I'll show that $L$ must be constant. \\ 
\\
Suppose that $L$ is not constant, i.e, there exists $y$ such that $Ly \neq Lx^*$. If $Ly < Lx^*$, then $x^*$ is not a minimizer and we have a contradiction. If $Ly > Lx^*$, then $L(x^* - y) < 0$ and we can consider the point $x^* + x^* - y$: 
\begin{align*}
L(x^* + x^* -y) = Lx^* + L(x^* - y) < Lx^*
\end{align*} 
so $x^*$ is not a minimizer and we have a contradiction
\end{proof}
\end{problem}

\begin{problem}{9.2}
\begin{proof}
Minimizing $||Ax-b||$ is equivalent to \begin{align*}
(Ax-b)^T(Ax-b) &= (x^TA^T - b^T)(Ax-b) \\ 
&= x^TA^TAx - x^TA^Tb -b^TAx +2b^Tb \\
&= x^TA^TAx -2b^TAx +2b^Tb
\end{align*}
Note that $A^TA$ is positive semidefinite. Taking the FOC of this expression yields:
\begin{align*}
2x^TA^TA - 2b^TA = 0 \\
\iff x^TA^TA = b^TA \\
\iff A^TAx = A^Tb
\end{align*}
And because $A^TA$ is positive definite, the second order-condition 
$$
2A^TA > 0
$$
will always be satisfied.
\end{proof}
\end{problem}

\begin{problem}{9.3}
Text explication - \textbf{Return here}
\end{problem}

\begin{problem}{9.4}
\begin{proof}
I'll need to show both directions.
\begin{itemize}
\item $\Leftarrow$ Suppose $x_0$ is chosen such that $Df(x_0)^T = Qx_0 - b$ is an eigenvector for $Q$, i.e, $Q(Qx_0-b) = \lambda (Qx_0-b)$ for some $\lambda \in \mathbb{R}$. We see that: 
\begin{align*}
x_1 = x_0 - \alpha Q(Qx_0 - b) = x_0 - \alpha \lambda (Qx_0 - b)
\end{align*}
I choose $\alpha$ to minimize $f(x_1)$. If $\alpha = \frac{1}{\lambda^2}$, then 
\begin{align*}
Qx_1 &= Q(x_0 - \alpha \lambda (Qx_0 - b)) \\ 
&= Qx_0 - \alpha\lambda^2(Qx_0 - b) \\
&= Qx_0 - Qx_0 - b\\
&= b
\end{align*}
And if $Qx_1 = b$, then we know that $x_1$ is the minimum of the function, so I've chosen the right $\alpha$, and the algorithm converges in one step.
\item $\Rightarrow$
Suppose the algorithm converges in one step. Then I know that $Qx_1 = b$, and thus that 
\begin{align*}
Q(x_0 - \alpha Q(Qx_0-b)) = b \\
\end{align*}
Now consider the kernel of $I - \alpha Q$.
\begin{align*}
(I- \alpha Q)(Qx_0 - b) &= Qx_0 - b - \alpha Q(Qx_0 - b) \\
&= Q(x_0 - \alpha Q(x_0-b)) - b \\
&= Qx_1 - b \\
&= 0
\end{align*}
So $Qx_1 - b \in$ Ker$(I- \alpha Q)$, and therefore it is an eigenvector of $Q$ with eigenvalue $\alpha$. \\
\end{itemize}
\end{proof} 
\textbf{I have two different eigenvalues for the same eigenvectors - one of the directions must be wrong. Anyone see the issue?}
\end{problem}

\begin{problem}{9.5}
\begin{proof}
I will begin by stating without proof a result of vector calculus. \\
\\ 
\textbf{Fact:} The gradient of a function at a point $Df^T(x)$ is orthogonal to the level set of the function at the point $x$. \\
\\
This fact gives some idea about where I'm going with this proof: first I'll show that I can reduce the proposition to the statement that the two gradients $Df^T(x_k)$ and $Df^T(x_{k+1})$ are orthogonal, and then I'll use the fact to show that this is indeed the case. \\
\\
Consider $\langle x_{k+1} - x_{k}, x_{k+2} - x_{k+1} \rangle$.
\begin{align*}
\langle x_{k+1} - x_{k}, x_{k+2} - x_{k+1} \rangle &= \langle x_{k} - \alpha_{k+1} Df^T(x_k) - x_{k}, x_{k+1} - \alpha_{k+2} Df^T(x_{k+1}) - x_{k+1} \rangle \\
&= \langle -\alpha_{k+1} Df^T(x_k), - \alpha_{k+2} Df^T(x_k+1) \rangle
\end{align*}
And if I want to set this equal to zero, I can pull out the scalars $-\alpha_{k+1}, -\alpha_{k+2}$ and set $\langle Df^T(x_k), Df^T(x_k+1) \rangle = 0$. So we see that 
$$
\langle x_{k+1} - x_{k}, x_{k+2} - x_{k+1} \rangle = 0 \iff \langle Df^T(x_k), Df^T(x_k+1) \rangle = 0
$$ 
I'll now show that the gradients are orthogonal. \\
\\
Consider the gradient $Df^T(x_k)$. $-Df^T(x_k)$ is the direction of steepest descent, and $x_{k+1} = x_k - \alpha Df^T(x_k)$. We choose $\alpha$ to minimize $f(x_{k+1})$. Consider the evaluation of the gradient $Df(x_k)$ at the point $x_{k+1}$. \\
\\
\textbf{Claim:} $Df(x_k)(x_{k+1}) = 0$\\
\\
\textit{Proof of Claim:} This will be an intuitive argument which follows from the continuity of the derivative ($f$ is $C^1$). Suppose $-Df(x_k)(x_{k+1}) < 0$. Then, I can go a bit further along the descent to 
$$x^* = x_k - (\alpha + \varepsilon ) Df^T(x_k), \varepsilon > 0$$
such that $f(x^*) < f(x_{k+1})$. Similarly, if $-Df(x_k)(x_{k+1}) > 0$, then I can go a bit less far along the descent to 
$$x^* = x_k - (\alpha - \varepsilon) Df^T(x_k), \varepsilon > 0$$
such that $f(x^*) < f(x_{k+1})$. So we see that $Df(x_k)(x_{k+1}) = 0$, which proves the claim. \\
\\
Excellent. Now, $Df(x_k)(x_{k+1}) = 0$, so the gradient $Df^T(x_k)$ is tangent to the level set of $f$ at the point $x_{k+1}$. We know from our fact that $Df^T(x_{k+1})$ is orthogonal to the level set of $f$ at $x_{k+1}$, so it is orthogonal to $Df^T(x_k)$ as well, which concludes the proof. \\
\\
See Figure 1  for some geometric intuition. \\
%\begin{figure} 
%\centering
%\includegraphics[width=.8\linewidth
%]{SD.jpg}
%\caption{A Nice Picture}
%\label{fig:boat1}
%\end{figure}
\end{proof}
\end{problem}

\begin{problem}{9.6}
Jupyter
\end{problem}

\begin{problem}{9.7}
Jupyter
\end{problem}

\begin{problem}{9.8}
Jupyter
\end{problem}

\begin{problem}{9.9}
Jupyter
\end{problem}

\begin{problem}{9.10} 
\begin{proof}
We know that $x^*$ is the unique minimizer of $f$ iff 
\begin{align*}
f'(x) = 0 &\iff Qx^* - b = 0 \iff x^* = Q^{-1}b 
\end{align*}
Now let us start Newton's method from an arbitrary initial guess $x_0$. Calculate $x_1$:
\begin{align*}
x_1 &= x_0 - D^2f(x_0)^{-1}Df(x_0) \\
&= x_0 - Q^{-1} (Qx_0 - b) \\
&= x_0 - x_0 + Q^{-1}b = Q^{-1}b
\end{align*}
which is what was desired.
\end{proof}
\end{problem}

\begin{problem}{9.12}
\begin{proof}
Choose $\lambda_i$ arbitrarily, and let $v_i$ be its eigenvector. Then
$$
Bv_i = (A + \mu I)v_i = Av_i + \mu I v_i = \lambda_i v_i + \mu v_i = (\lambda_i + \mu) v_i
$$
\end{proof}
\end{problem}

\begin{problem}{9.15} I'll multiply the left side by the right: 
\begin{proof}
\begin{align*}
(A+BCD)(A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1}) \\
= AA^{-1} - AA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
= I - B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
= I - B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
= I + BCDA^{-1} - (B(C^{-1} + DA^{-1}B)^{-1} + BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1})DA^{-1} \\
= I + BCDA^{-1} - ((B+BCDA^{-1}B)(C^{-1} + DA^{-1}B)^-{1}))DA^{-1} \\
= I + BCDA^{-1} - (BC(C^{-1}+DA^{-1}B)(C^{-1} + DA^{-1}B)^-{1}))DA^{-1} \\
= I + BCDA^{-1} - BCDA^{-1}
= I
\end{align*}

\end{proof}
\end{problem}

\begin{problem}{9.16}
I'm not seeing how to apply the theorem - \textbf{RETURN HERE}
\end{problem}

\begin{problem}{9.18}
\begin{proof}
I choose $\alpha_k$ to minimize the function $\phi_k(\alpha) = f(x_k + \alpha_k d_k)$, so I need $\phi_k'(\alpha_k) = 0$. Because $f$ is a quadratic, I know that 
\begin{align*}
\phi_k'(\alpha) &= - Df(x_k + \alpha_k d_k) \cdot d_k \\
&=[(x_k - \alpha_k d_k)^T Q - b^T] d_k \\
&= [x_k^TQ - b^T]d_k - (\alpha_kd_k)^TQd_k
= r_k^Td_k - \alpha_k(d_k^TQd_k)
\end{align*}
And from this last line we see:
$$
\alpha_k = \frac{r_k^Td_k}{d_k^TQd_k}
$$
\end{proof}
\end{problem}

\begin{problem}{9.20}
\begin{proof}
I will prove that $r_i^T r_k = 0$ for all $i < 0$ by induction on $k$. \\
\\
\textit{Base Case:} $k = 1$. Recall that in my proof from Problem 9.5, I showed that $Df^T(x_k)$ was orthogonal to $Df^T(x_{k+1})$, where $x_{k+1} = x_k - \alpha_k Df^T(x_k)$. In general the conjugate gradient method constructs $x_{k+1}$ differently, so this theorem does not always apply. But in the first step, $r_0 = d_0 = -Df(x_0)^T$, so we see that:
\begin{align*}
x_1 = x_0 + \alpha_0 d_0 = x_0 - \alpha_0 Df^T(x_0) \\
\implies r_1 = Df(x_1)^T \perp Df(x_0)^T = r_0 
\end{align*}
which shows the base case. \\
\\
\textit{Inductive Case:} Assume that 
$$r_i^T r_k' = 0 \text{ for all } i < k'$$ is true for any $k' < k$. I will show that the statement is also true for $k$. There is a bit of preliminary work necessary for my argument. Define the sets $D_{k-1} = $ span$\{d_0,...,d_{k-1}\}$ and $R_{k-1} = $ span$\{r_0, ... r_{k-1}\}$. I'll state and justify a few facts about these sets. \\
\\
\textbf{Fact 1}: $D_{k-1}$ and $R_{k-1}$ are both bases for subspaces of dimension $k-1$.\\
\textit{Justification}: $R_{k-1}$ and $D_{k-1}$ are both orthogonal over some inner product space on $R^{n}$: $R_{k-1}$ by the inductive assumption, and $D_{k-1}$ by the property that it is $Q$-conjugate (and so orthogonal over the inner product space $\langle \cdot, \cdot \rangle_Q$. It is a theorem somewhere that orthogonal vectors are linearly independent, which shows the fact. \\
\\
\textbf{Fact 2}: $D_{k-1} \subset R_{k-1}$ \\
\textit{Justification}: If $d \in D_{k+1}$, then it is a linear combination of elements $d_i, i \in \{0, 1, ..., k-1\}$. Therefore this fact will follow if I show that any element $d_i \in R_{k-1}$. And indeed, 
\begin{align*}
d_i &= r_{i} - \beta_{i-1}d_{i-1} \\
&= r_i - \beta_{i-1}(r_{i-1} - \beta_{i-2}d_{i-2}) = r_i - \beta_{i-1} r_{i-1} + \beta_{i-1} \beta_{i-2} d_{i-2} \\
&= ... \\
&= \sum_{j=0}^i \bigg( \prod_{k=j}^{i-1} -\beta_{k} \bigg) r_j 
\end{align*}
The actual final expression doesn't matter - what matters is that $d_i$ is expressed as a linear combination of the $r_i$s. 
\\
\\
\textbf{Fact 3}: $D_{k+1} = R_{k+1}$ \\
\textit{Justification}: This follows from facts 1 and 2: Since the two spaces have the same dimension, one inclusion implies equality. \\
\\
Now, we'll put this new knowledge to work and prove the inductive step. By Lemma 9.5.3, $d_i^Tr_k = 0$ for any $i < j$. This means that $r_k \in D_{k-1}^\perp = R_{k-1}^\perp$ with the usual inner product, and therefore $r_i^Tr_k = 0$ for all $i < k$, which was what we wanted to show. 
\end{proof}
\end{problem}
\textit{Note: There has to be a simpler way to do this proof. Apologies if this was very roundabout, but it's all I could think of!}
\end{document}